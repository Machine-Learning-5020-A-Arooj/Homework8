{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework08\n",
    "\n",
    "Exercises to practice pandas, data analysis and classification\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand the effects of pre-processing data\n",
    "- Get familiar with the ML flow: encode -> normalize -> train -> evaluate\n",
    "- Understand the difference between regression and classification tasks\n",
    "- Build intuition for different classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/image_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/0801-500.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PIL.Image as PImage\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from data_utils import classification_error, display_confusion_matrix, regression_error\n",
    "\n",
    "from image_utils import get_pixels, make_image\n",
    "\n",
    "from Homework08_utils import CamUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The dataset we are going to use has images from $25$ different security cameras, and our task is to separate them by camera. Some of the cameras move, some of them don't, and there are more than $1000$ images, so there's no way we want to do this by hand.\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "If we look at the images in `./data/image/0801-500/train/`, we'll notice that they are named and organized in a very particular way. They're all in the same directory and the first part of their filename specifies which camera they came from. Even though those `ids` are numbers, they're not sequential, so we'll use some helper functions to extract a unique `label` from their filenames.\n",
    "\n",
    "This is exactly what the `OrdinalEncoder` class does, but since we only have to encode this one column, we'll do it by hand while we read the files in.\n",
    "\n",
    "Alternatively, we could try using a `OneHotEncoder`, but since we have $25$ cameras, adding $25$ sparse columns to our dataset might confuse our classification fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a list of all the files in a given directory, that end in .jpg\n",
    "train_files = [f for f in listdir(\"./data/image/0801-500/train\") if f.endswith(\".jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: check and see what is inside the list here\n",
    "\n",
    "print(\"Number of training images:\", len(train_files))\n",
    "\n",
    "train_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll read the image pixels and extract their labels. `CamUtils.get_label()` is the helper function we'll use to \"encode\" and return a label id based on the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_data = []\n",
    "label_data = []\n",
    "\n",
    "for fname in train_files:\n",
    "  label = CamUtils.get_label(fname)\n",
    "  img = PImage.open(path.join(\"./data/image/0801-500/train\", fname))\n",
    "  pixel_data.append(get_pixels(img))\n",
    "  label_data.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: check if labels got extracted correctly by looking at \n",
    "#       the first few items of the label list and the filename list\n",
    "print(\"First 10 image filenames:\")\n",
    "print(train_files[:10])\n",
    "\n",
    "print(\"First 10 extracted labels:\")\n",
    "print(label_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels and the filenames won't match exactly since labels start at $0$ and the filenames start at $01$ and skip some numbers.\n",
    "\n",
    "We can open some images from pixels, just to make sure we loaded them correctly (they're squares, so no need to specify `width`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(pixel_data[0]))\n",
    "display(make_image(pixel_data[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now might not be a bad time to peek into the `data/image/0801-500/` directories to see what's inside them and what the images look like.... and get to know the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame it\n",
    "\n",
    "Let's put our raw pixel data into a `DataFrame`, and create a column for storing each image's label.\n",
    "\n",
    "(this next cell might take a while to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(pixel_data)\n",
    "train_df[\"label\"] = label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "Does anything stand out as peculiar about the feature values in our <code>DataFrame</code>?<br>\n",
    "Do we have to encode or scale our data?<br>\n",
    "Why? Or, why not?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>\n",
    "Looking at the DataFrame, all the feature columns contain pixel intensity values in the 0‚Äì255 range. Since every feature represents the same kind of measurement and everything is already numeric, nothing looks inconsistent or mixed. The labels we extracted are also integers, so there‚Äôs no categorical text that would require encoding.\n",
    "\n",
    "Because the pixel values are already on the same scale and represent the same type of data, there isn‚Äôt an obvious need to normalize or transform them further. The dataset seems clean and ready to be used by a classifier without additional preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Files\n",
    "\n",
    "If that worked, repeat the process for the test files inside the `./data/image/0801-500/test/` directory.\n",
    "\n",
    "We can almost use the exact same steps as we did above to create a `DataFrame`, the only difference being that we don't have labels for these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create a list of files in the test/ directory\n",
    "test_files = [f for f in listdir(\"./data/image/0801-500/test\") if f.endswith(\".jpg\")]\n",
    "\n",
    "# TODO: check its length and content\n",
    "print(\"Number of test images:\", len(test_files))\n",
    "print(test_files[:10])\n",
    "\n",
    "test_pixel_data = []\n",
    "\n",
    "# TODO: loop over files and load their pixels into a list\n",
    "test_pixel_data = []\n",
    "\n",
    "for fname in test_files:\n",
    "    img = PImage.open(path.join(\"./data/image/0801-500/test\", fname))\n",
    "    test_pixel_data.append(get_pixels(img))\n",
    "\n",
    "# TODO: load into DataFrame (this might take 20 - 30 seconds)\n",
    "test_df = pd.DataFrame(test_pixel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like data!!\n",
    "\n",
    "We could train a `RandomForestClassifier` directly on this `DataFrame` and see what would happen, but my guess is that Python runs out of memory and crashes our tab/browser/computer...\n",
    "\n",
    "We'll use _projection_ to reduce the number of dimensions in our dataset. Projection is when we just drop some of the columns in our dataset. \n",
    "\n",
    "Which columns ? That's up to us.\n",
    "\n",
    "Let's first try using the first $N$ columns/features where $N$ is a number around $10$.\n",
    "\n",
    "This is how we get the first $N$ columns from a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split input and output features\n",
    "NUM_FEATURES = 10\n",
    "chosen_columns = train_df.columns[:NUM_FEATURES]\n",
    "train_features = train_df[chosen_columns]\n",
    "\n",
    "out_features = train_df[\"label\"]\n",
    "\n",
    "# also separate test dataset features\n",
    "test_features = test_df[chosen_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our [Week 08](https://github.com/PSAM-5020-2025F-A/WK08) notebook, we can create a classification model by following these steps:\n",
    "\n",
    "1. Load dataset (done! üéâ)\n",
    "2. Encode label features as numbers (not needed! done! ‚ö°Ô∏è)\n",
    "3. Normalize the data (not needed! done! üçæ)\n",
    "4. Separate the outcome variable and the input features (done! ‚òÄÔ∏è)\n",
    "5. Create a model using chosen features\n",
    "6. Run model on training data and measure error*\n",
    "7. Run model on test data, measure error*, plot predictions, interpret results\n",
    "\n",
    "We could use the same `regression_error()` function we used previously to measure the error of our classifier model, but this could lead to $2$ issues. First, we don't have labels for the images in the test dataset, and second, the regression error reported might be higher than it actually is because an image with label $0$ that gets mislabeled as $5$ will count as being more wrong than if it was mislabeled $2$. And we don't want that. We just want to get the percentage of classifications that our model gets correctly.\n",
    "\n",
    "To simplify calculating the classification accuracy we can use the `CamUtils.classification_accuracy()` function. This function takes $2$ parameters, a list of files and a list of predictions. It will work with the test and train datasets and will calculate a more meaningful accuracy value than the one returned by `regression_error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create a brand new classifier\n",
    "clf = RandomForestClassifier(random_state=1010)\n",
    "\n",
    "# TODO: fit the model\n",
    "clf.fit(train_features, out_features)\n",
    "\n",
    "# TODO: run predictions\n",
    "train_predictions = clf.predict(train_features)\n",
    "\n",
    "# TODO: measure classification accuracy\n",
    "CamUtils.classification_accuracy(train_files, train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look promising. Let's run this on our test dataset.\n",
    "\n",
    "Remember we already separated the test data features into a variable called `test_features` above.\n",
    "\n",
    "Now we just have to run the prediction and measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: run predictions on test data\n",
    "test_predictions = clf.predict(test_features)\n",
    "\n",
    "# TODO: measure classification accuracy\n",
    "CamUtils.classification_accuracy(test_files, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Using just the first $10$ pixels of the image the classifier is able to label most of the images correctly.\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How can we improve this classifier? How does the number of features affect the classification accuracy of the test data?<br>\n",
    "How does the choice of pixels affect the accuracy?<br><br>\n",
    "If you're curious, repeat the modeling above, but using the <code>SVC</code> classifier instead of <code>RandomForest</code>.<br>How does the choice of modeling technique affect the accuracy?<br><br>\n",
    "Experiment with some of these parameters and explain your findings below.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>\n",
    "Right now we‚Äôre only using the first 10 pixels of each image and even with that tiny amount of information the model still gets around 65% accuracy on the test set. That‚Äôs actually pretty decent considering the model barely sees any part of the image. A straightforward way to improve this is to increase the number of pixels we allow the classifier to use. Since we limited it to just 10 columns on purpose, giving it more features (like 20, 50 or even 100 pixels) would let the model see more of the image and usually helps it tell the cameras apart better.\n",
    "\n",
    "The specific pixels we choose also matter a lot. Using the first 10 pixels is kind of random and they might come from an area of the image that doesn‚Äôt look very different across cameras. If those pixels happen to be from a part of the scene that‚Äôs similar everywhere (like sky or some blank region), the model won‚Äôt learn much from them. If we pick pixels from other parts of the image or spread them out more, we‚Äôre more likely to capture something unique to each camera. So both the number of pixels and where they come from can make a noticeable difference in the classifier‚Äôs accuracy on new images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "## 1. Create an SVC model\n",
    "svc_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "\n",
    "## 2. Fit the model on the training data\n",
    "svc_model.fit(train_features, out_features)\n",
    "\n",
    "## 3. Predict on the training data\n",
    "svc_train_predictions = svc_model.predict(train_features)\n",
    "\n",
    "## 4. Predict on the test data\n",
    "svc_test_predictions = svc_model.predict(test_features)\n",
    "\n",
    "## 5. Measure and print accuracies\n",
    "svc_train_acc = CamUtils.classification_accuracy(train_files, svc_train_predictions)\n",
    "svc_test_acc  = CamUtils.classification_accuracy(test_files, svc_test_predictions)\n",
    "\n",
    "print(\"Training accuracy:\", svc_train_acc)\n",
    "print(\"Test accuracy:\", svc_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same 10-pixel slice, the SVC model performs noticeably worse than the RandomForest model. The SVC reaches about 44% accuracy on the training data and about 43% on the test data. This means it‚Äôs not really overfitting, but it‚Äôs also not learning very strong boundaries from such a small set of features. In contrast, the RandomForest classifier was able to fit the training data perfectly and still reach around 65% on the test set.\n",
    "\n",
    "This difference shows that the choice of modeling technique matters a lot when we‚Äôre only using a tiny number of pixels. RandomForest can handle small feature sets and pick up simple brightness patterns more easily, while SVC seems to struggle with separating the classes based on just 10 values. With more pixels or different hyperparameters, SVC might do better, but with the current setup it‚Äôs not as effective as the RandomForest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = [20, 50, 100]\n",
    "\n",
    "for NUM_FEATURES in feature_counts:\n",
    "    print(f\"\\nUsing NUM_FEATURES = {NUM_FEATURES}\")\n",
    "    \n",
    "    \n",
    "    chosen_columns = train_df.columns[:NUM_FEATURES]\n",
    "    train_features = train_df[chosen_columns]\n",
    "    test_features  = test_df[chosen_columns]\n",
    "    \n",
    "    #RandomForest\n",
    "    rf_model = RandomForestClassifier(random_state=1010)\n",
    "    rf_model.fit(train_features, out_features)\n",
    "    \n",
    "    rf_train_pred = rf_model.predict(train_features)\n",
    "    rf_test_pred  = rf_model.predict(test_features)\n",
    "    \n",
    "    rf_train_acc = CamUtils.classification_accuracy(train_files, rf_train_pred)\n",
    "    rf_test_acc  = CamUtils.classification_accuracy(test_files,  rf_test_pred)\n",
    "    \n",
    "    print(\"RandomForest - Train acc:\", rf_train_acc)\n",
    "    print(\"RandomForest - Test  acc:\", rf_test_acc)\n",
    "    \n",
    "    #SVC\n",
    "    svc_model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "    svc_model.fit(train_features, out_features)\n",
    "    \n",
    "    svc_train_pred = svc_model.predict(train_features)\n",
    "    svc_test_pred  = svc_model.predict(test_features)\n",
    "    \n",
    "    svc_train_acc = CamUtils.classification_accuracy(train_files, svc_train_pred)\n",
    "    svc_test_acc  = CamUtils.classification_accuracy(test_files,  svc_test_pred)\n",
    "    \n",
    "    print(\"SVC - Train acc:\", svc_train_acc)\n",
    "    print(\"SVC - Test  acc:\", svc_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I increased the number of pixels from 20 to 50 to 100, the RandomForest model consistently improved on the test set. At 20 pixels it was around 0.69, then it jumped to about 0.78 with 50 pixels and reached around 0.84 when using 100 pixels. The training accuracy stayed at 1.0 the whole time, which suggests that the model can easily fit the training data once it has enough features, but the steady rise in test accuracy shows that giving the model more of the image really does help it generalize better.\n",
    "\n",
    "For SVC, the pattern was similar but much less dramatic. Its performance did improve when I added more features, but not as strongly as the RandomForest. At 20 pixels it was still around 0.45 on the test set, with 50 pixels it went up to about 0.52 and at 100 pixels it reached around 0.58. So SVC definitely benefits from having more pixels, but it still lags behind RandomForest by a noticeable amount at every feature level. Overall, increasing the number of pixels helped both models, but RandomForest gained much more from the extra information than SVC did. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
